{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNoHZAAB15z+BUztFWVAIi3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vennyx14/Finbert_SOTA/blob/main/FinBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Setup & Config**\n"
      ],
      "metadata": {
        "id": "YaYBH8_ZTjUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xYN6gvFRrJpV"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch pandas scikit-learn tqdm\n",
        "\n",
        "import random, os, math, time\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import warnings\n",
        "\n",
        "import os\n",
        "import ast\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    logging\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.set_verbosity_warning()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value=42):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = \"/content/drive/My Drive/FinBERT_Crypto_Project\"\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "class Config:\n",
        "    MODEL_NAME = \"ProsusAI/finbert\"\n",
        "\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 16\n",
        "    VALID_BATCH_SIZE = 16\n",
        "    EPOCHS = 8\n",
        "    LR = 2e-5\n",
        "\n",
        "    ACCUM_STEPS = 2\n",
        "\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    WARMUP_RATIO = 0.1\n",
        "    NUM_LABELS = 3\n",
        "    OUTPUT_DIR = drive_path\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    SEED = 42\n",
        "\n",
        "print(\"Device:\", Config.DEVICE)"
      ],
      "metadata": {
        "id": "NLbxKpPHredk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Data Preparation**"
      ],
      "metadata": {
        "id": "Ou364yKhTt6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalize & Load dataset"
      ],
      "metadata": {
        "id": "HDR84ji1mETb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_labels(series):\n",
        "    \"\"\"\n",
        "    Normalize labels to:\n",
        "    0 = negative, 1 = neutral, 2 = positive\n",
        "    \"\"\"\n",
        "    # Case 1: string labels\n",
        "    if series.dtype == object:\n",
        "        s = series.astype(str).str.lower().str.strip()\n",
        "\n",
        "        label_map = {\n",
        "            \"negative\": 0,\n",
        "            \"neg\": 0,\n",
        "            \"bearish\": 0,\n",
        "\n",
        "            \"neutral\": 1,\n",
        "            \"neu\": 1,\n",
        "\n",
        "            \"positive\": 2,\n",
        "            \"pos\": 2,\n",
        "            \"bullish\": 2\n",
        "        }\n",
        "\n",
        "        mapped = s.map(label_map)\n",
        "\n",
        "        if mapped.isna().any():\n",
        "            bad = s[mapped.isna()].unique()\n",
        "            raise ValueError(f\"Unknown label values: {bad}\")\n",
        "\n",
        "        return mapped.astype(int)\n",
        "\n",
        "    # Case 2: numeric labels\n",
        "    else:\n",
        "        unique_vals = sorted(series.unique())\n",
        "\n",
        "        if set(unique_vals) == {0, 1, 2}:\n",
        "            return series.astype(int)\n",
        "\n",
        "        if set(unique_vals) == {-1, 0, 1}:\n",
        "            return series.map({-1: 0, 0: 1, 1: 2}).astype(int)\n",
        "\n",
        "        raise ValueError(f\"Unrecognized numeric labels: {unique_vals}\")\n"
      ],
      "metadata": {
        "id": "lTra9UhNjBoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  df = pd.read_csv(path)\n",
        "\n",
        "  df.columns = [c.lower().strip() for c in df.columns]\n",
        "\n",
        "  # find text col\n",
        "  if \"text\" in df.columns:\n",
        "    pass\n",
        "  elif \"sentence\" in df.columns:\n",
        "    df.rename(columns={\"sentence\": \"text\"}, inplace=True)\n",
        "\n",
        "  # find label col\n",
        "  if \"label\" in df.columns:\n",
        "    pass\n",
        "  elif \"sentiment\" in df.columns:\n",
        "    df.rename(columns={\"sentiment\": \"label\"}, inplace=True)\n",
        "\n",
        "  df = df.dropna(subset=[\"text\", \"label\"])\n",
        "\n",
        "  df[\"text\"] = (\n",
        "        df[\"text\"]\n",
        "        .astype(str)\n",
        "        .str.replace(\"\\n\", \" \", regex=False)\n",
        "        .str.strip()\n",
        "    )\n",
        "\n",
        "  df[\"label\"] = normalize_labels(df[\"label\"])\n",
        "  df[\"label\"] = df[\"label\"].astype(int)\n",
        "\n",
        "  return df\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
        "\n",
        "# Dataset\n",
        "class CryptoDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = list(texts)\n",
        "        self.labels = list(labels)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        txt = str(self.texts[idx])\n",
        "        enc = self.tokenizer(\n",
        "            txt,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\": enc[\"input_ids\"],\n",
        "            \"attention_mask\": enc[\"attention_mask\"],\n",
        "            \"labels\": int(self.labels[idx])\n",
        "        }\n",
        "\n",
        "        if \"token_type_ids\" in enc:\n",
        "            item[\"token_type_ids\"] = enc[\"token_type_ids\"]\n",
        "        return item\n",
        "\n",
        "# Collate: dynamic padding + trả tensors\n",
        "def collate_fn(batch):\n",
        "    # batch: list of dicts with lists for ids/mask\n",
        "    input_ids = [b[\"input_ids\"] for b in batch]\n",
        "    attention_mask = [b[\"attention_mask\"] for b in batch]\n",
        "    to_pad = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "    if \"token_type_ids\" in batch[0]:\n",
        "        to_pad[\"token_type_ids\"] = [b[\"token_type_ids\"] for b in batch]\n",
        "\n",
        "    padded = tokenizer.pad(\n",
        "    to_pad,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
        "\n",
        "    padded[\"labels\"] = labels\n",
        "    return padded\n"
      ],
      "metadata": {
        "id": "4f9vzlE4URt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test"
      ],
      "metadata": {
        "id": "EG-YmtGYl5ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_finance = load_data(\"/data.csv\")\n",
        "print(f\"Finance loaded: {len(df_finance)} rows\")\n",
        "\n",
        "\n",
        "# XỬ LÝ DỮ LIỆU CRYPTO\n",
        "df_crypto = pd.read_csv(\"/cryptonews.csv\")\n",
        "\n",
        "def clean_sentiment_col(row):\n",
        "\n",
        "    if isinstance(row, str) and \"{\" in row:\n",
        "        try:\n",
        "            data_dict = ast.literal_eval(row)\n",
        "            return data_dict.get('class')\n",
        "        except:\n",
        "            return None\n",
        "    return row\n",
        "\n",
        "sent_col = 'sentiment' if 'sentiment' in df_crypto.columns else 'Sentiment'\n",
        "df_crypto['label'] = df_crypto[sent_col].apply(clean_sentiment_col)\n",
        "\n",
        "# normalize_labels\n",
        "df_crypto['label'] = normalize_labels(df_crypto['label'])\n",
        "df_crypto = df_crypto[['text', 'label']]\n",
        "\n",
        "print(f\"Crypto loaded: {len(df_crypto)} rows\")\n",
        "\n",
        "\n",
        "# Gộp & # Xáo trộn ngẫu nhiên\n",
        "df = pd.concat([df_finance, df_crypto], ignore_index=True)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"TỔNG DỮ LIỆU SAU GỘP: {len(df)}\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"-\" * 30)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "N9wl45jCer7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"text\"],\n",
        "    df[\"label\"],\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label\"],\n",
        "    random_state=Config.SEED\n",
        ")\n",
        "\n",
        "train_dataset = CryptoDataset(\n",
        "    train_texts,\n",
        "    train_labels,\n",
        "    tokenizer,\n",
        "    Config.MAX_LEN\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=Config.TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print(\"input_ids shape:\", batch[\"input_ids\"].shape)\n",
        "print(\"labels shape:\", batch[\"labels\"].shape)\n",
        "\n",
        "# Tạo Dataset cho tập Validation\n",
        "val_dataset = CryptoDataset(\n",
        "    val_texts,\n",
        "    val_labels,\n",
        "    tokenizer,\n",
        "    Config.MAX_LEN\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=Config.VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Số batch trong tập Train: {len(train_loader)}\")\n",
        "print(f\"Số batch trong tập Val: {len(val_loader)}\")\n"
      ],
      "metadata": {
        "id": "j93xzW57Y8hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Model Setup**"
      ],
      "metadata": {
        "id": "i-MfZD01T267"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load FinBERT + Classification Head"
      ],
      "metadata": {
        "id": "J8p3zvmQlsFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        Config.MODEL_NAME,\n",
        "        num_labels=Config.NUM_LABELS\n",
        "    )\n",
        "\n",
        "    model.to(Config.DEVICE)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model()"
      ],
      "metadata": {
        "id": "pW9B66KqUSIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KJuqNXHem5ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optimizer"
      ],
      "metadata": {
        "id": "f4sqZkV5mVKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n,p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": Config.WEIGHT_DECAY,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n,p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=Config.LR,\n",
        "    eps=1e-8\n",
        ")\n"
      ],
      "metadata": {
        "id": "uLs7QD2xlW51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Warmup & Scheduler Setup"
      ],
      "metadata": {
        "id": "zdaxNXcwmkNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = int(len(train_loader) * Config.EPOCHS / Config.ACCUM_STEPS)\n",
        "num_warmup_steps = int(Config.WARMUP_RATIO * num_training_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "6WYGf6KxlXOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AMP-Mixed precision"
      ],
      "metadata": {
        "id": "tkwdwNXBmq-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "print(f\"Total training steps: {num_training_steps}\")\n",
        "print(\"Model Setup Complete.\")"
      ],
      "metadata": {
        "id": "y6PrPgj2mst6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Training Engine**\n"
      ],
      "metadata": {
        "id": "lIlTZiECT7il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(\n",
        "            logits,\n",
        "            targets,\n",
        "            weight=self.weight,\n",
        "            reduction=\"none\"\n",
        "        )\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "19TMJ9xYyvua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Helpers\n",
        "def get_lr(optimizer):\n",
        "    for g in optimizer.param_groups:\n",
        "        return g['lr']\n",
        "\n",
        "def save_checkpoint(model, tokenizer, optimizer, scheduler, epoch, path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    # Save model + tokenizer + optimizer + scheduler states\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n",
        "    }, os.path.join(path, \"training_state.pt\"))\n",
        "    # Saving model & tokenizer for inference\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "def load_checkpoint(path, model, optimizer=None, scheduler=None, device=None):\n",
        "    ckpt = torch.load(os.path.join(path, \"training_state.pt\"), map_location=device)\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    if optimizer is not None and ckpt.get('optimizer_state_dict') is not None:\n",
        "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "    if scheduler is not None and ckpt.get('scheduler_state_dict') is not None:\n",
        "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "    return ckpt.get('epoch', None)\n",
        "\n",
        "\n",
        "def to_device_batch(batch, device):\n",
        "    # Move all tensors in batch dict to device\n",
        "    for k, v in batch.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            batch[k] = v.to(device)\n",
        "    return batch\n",
        "\n",
        "def train_one_epoch(model,dataloader,optimizer,criterion,scheduler=None,epoch=0,\n",
        "                    scaler=None,accumulation_steps=1,device=None,max_grad_norm=1.0):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    step = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pbar = tqdm(dataloader, desc=f\"Train E{epoch}\")\n",
        "\n",
        "    for i, batch in enumerate(pbar):\n",
        "        step += 1\n",
        "\n",
        "        # move batch to device\n",
        "        batch = to_device_batch(batch, device)\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch.get(\"token_type_ids\", None)\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "        # backward\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        # update\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            if scaler is not None:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "        pbar.set_postfix(\n",
        "            loss=f\"{total_loss/step:.4f}\",\n",
        "            lr=f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
        "        )\n",
        "\n",
        "    return total_loss / step\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, device=None):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    probs_all = []\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Eval\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in pbar:\n",
        "            batch = to_device_batch(batch, device)\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            token_type_ids = batch.get('token_type_ids', None)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            pred = torch.argmax(probs, dim=-1)\n",
        "\n",
        "            preds.extend(pred.detach().cpu().tolist())\n",
        "            trues.extend(labels.detach().cpu().tolist())\n",
        "            probs_all.extend(probs.detach().cpu().tolist())\n",
        "\n",
        "    # Metrics\n",
        "    weighted_f1 = f1_score(trues, preds, average='weighted')\n",
        "    macro_f1 = f1_score(trues, preds, average='macro')\n",
        "    report = classification_report(trues, preds, digits=4, output_dict=True)\n",
        "    cm = confusion_matrix(trues, preds)\n",
        "    return {\n",
        "        'weighted_f1': weighted_f1,\n",
        "        'macro_f1': macro_f1,\n",
        "        'report': report,\n",
        "        'confusion_matrix': cm,\n",
        "        'preds': preds,\n",
        "        'trues': trues,\n",
        "        'probs': probs_all\n",
        "    }\n"
      ],
      "metadata": {
        "id": "cG1CBovGUSe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "\n",
        "def train_loop(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    epochs=Config.EPOCHS,\n",
        "    accumulation_steps=Config.ACCUM_STEPS,\n",
        "    class_weights=None,\n",
        "    patience=3,\n",
        "    device=Config.DEVICE,\n",
        "    output_dir=Config.OUTPUT_DIR,\n",
        "    max_grad_norm=1.0,\n",
        "    use_amp=True\n",
        "):\n",
        "    scaler = torch.cuda.amp.GradScaler() if use_amp and device.type == 'cuda' else None\n",
        "\n",
        "\n",
        "    if class_weights is not None:\n",
        "        criterion = FocalLoss(\n",
        "            gamma=2.0,\n",
        "            weight=class_weights.to(device)\n",
        "        )\n",
        "    else:\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_f1 = -1\n",
        "    best_epoch = -1\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train_one_epoch(\n",
        "            model=model,\n",
        "            dataloader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            epoch=epoch,\n",
        "            scaler=scaler,\n",
        "            accumulation_steps=accumulation_steps,\n",
        "            device=device,\n",
        "            max_grad_norm=max_grad_norm\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_metrics = evaluate(model, val_loader, device=device)\n",
        "        val_macro_f1 = val_metrics[\"macro_f1\"]\n",
        "\n",
        "        print(f\"\\nEpoch {epoch}\")\n",
        "        print(f\"Train loss: {train_loss:.4f}\")\n",
        "        print(f\"Val macro F1: {val_macro_f1:.4f}\")\n",
        "\n",
        "        if val_macro_f1 > best_val_f1:\n",
        "            best_val_f1 = val_macro_f1\n",
        "            best_epoch = epoch\n",
        "            epochs_no_improve = 0\n",
        "\n",
        "            ckpt_path = os.path.join(\n",
        "                output_dir,\n",
        "                f\"best_model_epoch{epoch}_macroF1{val_macro_f1:.4f}\"\n",
        "            )\n",
        "            save_checkpoint(model, tokenizer, optimizer, scheduler, epoch, ckpt_path)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    return best_val_f1, train_losses\n"
      ],
      "metadata": {
        "id": "22HnxU-955xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Compute class weights =====\n",
        "train_labels_array = np.array(train_labels)\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(train_labels_array),\n",
        "    y=train_labels_array\n",
        ")\n",
        "\n",
        "class_weights_tensor = torch.tensor(\n",
        "    class_weights, dtype=torch.float\n",
        ").to(Config.DEVICE)\n",
        "\n",
        "print(f\"Class Weights: {class_weights_tensor}\")\n",
        "\n",
        "# ===== Train =====\n",
        "print(f\"\\n TRAINING FOR {Config.EPOCHS} EPOCHS\")\n",
        "\n",
        "best_f1_score, train_losses = train_loop(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    epochs=Config.EPOCHS,\n",
        "    accumulation_steps=Config.ACCUM_STEPS,\n",
        "    class_weights=class_weights_tensor,\n",
        "    patience=3,\n",
        "    device=Config.DEVICE,\n",
        "    output_dir=Config.OUTPUT_DIR\n",
        ")\n",
        "\n",
        "print(f\"\\n FINAL RESULT — Best Macro F1: {best_f1_score:.4f}\")"
      ],
      "metadata": {
        "id": "oUgrEPf_IPKN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(Config.DEVICE)\n",
        "            attention_mask = batch[\"attention_mask\"].to(Config.DEVICE)\n",
        "            labels = batch[\"labels\"].to(Config.DEVICE)\n",
        "\n",
        "            token_type_ids = batch.get(\"token_type_ids\")\n",
        "            if token_type_ids is not None:\n",
        "                token_type_ids = token_type_ids.to(Config.DEVICE)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            real_values.extend(labels)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu().numpy()\n",
        "    real_values = torch.stack(real_values).cpu().numpy()\n",
        "    return predictions, real_values\n",
        "\n",
        "y_pred, y_test = get_predictions(model, val_loader)\n",
        "\n",
        "class_names = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "print(\"FINAL CLASSIFICATION REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "qIS-j2_QyALc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Evaluation & Analysis**"
      ],
      "metadata": {
        "id": "bXjv4JvuUDxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_trues = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_trues.extend(batch[\"labels\"].cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_trues = np.array(all_trues)\n",
        "all_probs = np.array(all_probs)\n"
      ],
      "metadata": {
        "id": "yxbMQ6tcRKtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(\n",
        "    all_trues,\n",
        "    all_preds,\n",
        "    target_names=[\"negative\", \"neutral\", \"positive\"],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "cm = confusion_matrix(all_trues, all_preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bJVlGDQuUS3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_idx = np.where(all_preds != all_trues)[0]\n",
        "\n",
        "# Sắp xếp theo độ tự tin của dự đoán sai (max prob)\n",
        "confidence = all_probs[wrong_idx].max(axis=1)\n",
        "top_wrong = wrong_idx[np.argsort(-confidence)][:10]\n",
        "\n",
        "for i in top_wrong:\n",
        "    print(\"TEXT:\")\n",
        "    print(val_texts.iloc[i])\n",
        "    print(f\"TRUE: {all_trues[i]} | PRED: {all_preds[i]} | PROBS: {all_probs[i]}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "-wvDAgZEv6K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VẼ BIỂU ĐỒ LOSS\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_losses, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.grid(True)\n",
        "plt.savefig('loss_table.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ppgs98RT09Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "xF7IXgueB7kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('loss_table.png')\n",
        "files.download('confusion_matrix.png')"
      ],
      "metadata": {
        "id": "9ds11J7qB8bs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}